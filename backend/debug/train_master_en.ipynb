{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6f289b",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from IPython.display import Markdown\n",
    "from IPython.display import display\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import glob\n",
    "import chromadb\n",
    "from chromadb import Documents, EmbeddingFunction, Embeddings\n",
    "from google.api_core import retry\n",
    "from langcodes import Language\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from typing import List, Tuple\n",
    "import fasttext\n",
    "import re\n",
    "from typing import Any, Dict, List, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad699fd",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def import_google_api():\n",
    "    load_dotenv()\n",
    "    GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "    client = genai.Client(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "    # Simple model check (keeping the original print logic)\n",
    "    for m in client.models.list():\n",
    "        if \"embedContent\" in m.supported_actions:\n",
    "            print(m.name)\n",
    "\n",
    "    return client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4171d9",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def embedding_function(client):\n",
    "    class GeminiEmbeddingFunction(EmbeddingFunction):\n",
    "        document_mode = True\n",
    "\n",
    "        def __init__(self, client):\n",
    "            self.client = client\n",
    "            # Retry only on specific transient API errors\n",
    "            self._retry = retry.Retry(predicate=lambda e: isinstance(e, genai.errors.APIError) and e.code in {429, 503})\n",
    "\n",
    "        def __call__(self, input: Documents) -> Embeddings:\n",
    "            embedding_task = \"retrieval_document\" if self.document_mode else \"retrieval_query\"\n",
    "            response = self._retry(self.client.models.embed_content)(\n",
    "                model=\"models/text-embedding-004\",\n",
    "                contents=input,\n",
    "                config=types.EmbedContentConfig(task_type=embedding_task),\n",
    "            )\n",
    "            return [e.values for e in response.embeddings]\n",
    "\n",
    "    return GeminiEmbeddingFunction(client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fb3f4f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Assuming 'Document' is a class/dataclass with 'page_content' and 'metadata' attributes\n",
    "# from a library like LangChain, LlamaIndex, etc.\n",
    "class Document:\n",
    "    def __init__(self, page_content: str, metadata: dict = None):\n",
    "        self.page_content = page_content\n",
    "        self.metadata = metadata if metadata is not None else {}\n",
    "\n",
    "# NOTE: The original doc_stats, filename_base, google_drive_path, and all_chunks \n",
    "# are not defined in the provided snippet. I'll define placeholders or \n",
    "# make assumptions for a runnable example.\n",
    "\n",
    "'''def parse_markdown_for_metadata(directory: str, google_drive_path: str = None) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Reads markdown files in a directory (and subdirectories) and creates a \n",
    "    single Document for each file, adding relevant metadata, but does not chunk.\n",
    "    \"\"\"\n",
    "    markdown_files = glob.glob(os.path.join(directory, '**/*.md'), recursive=True)\n",
    "    if not markdown_files:\n",
    "        print(\"No markdown files found\")\n",
    "        return []\n",
    "\n",
    "    print(f\"Processing {len(markdown_files)} markdown files...\")\n",
    "    \n",
    "    all_documents = []\n",
    "\n",
    "    for filepath in tqdm(markdown_files, desc=\"Processing documents\"):\n",
    "        try:\n",
    "            with open(filepath, 'r', encoding='utf-8') as f:\n",
    "                markdown_text = f.read()\n",
    "        except Exception as e:\n",
    "            print(f\"\\nWarning: Could not read file {filepath}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Extract basic file name for metadata\n",
    "        filename_base = os.path.basename(filepath)\n",
    "\n",
    "        # Create a single Document for the entire file content\n",
    "        doc = Document(page_content=markdown_text)\n",
    "\n",
    "        # Add metadata\n",
    "        doc.metadata[\"source\"] = filename_base\n",
    "        # Use google_drive_path if provided, otherwise use local path\n",
    "        doc.metadata[\"source_path\"] = google_drive_path or filepath \n",
    "        \n",
    "        # NOTE: Since we are not chunking by headers, we won't have a specific header.\n",
    "        # We set it to an empty string or a placeholder.\n",
    "        doc.metadata[\"header\"] = \"\" \n",
    "        \n",
    "        # Since the entire file is one document, these values reflect that.\n",
    "        doc.metadata[\"chunk_index\"] = 0\n",
    "        doc.metadata[\"total_chunks\"] = 1\n",
    "        doc.metadata[\"is_complete_doc\"] = True\n",
    "        \n",
    "        all_documents.append(doc)\n",
    "\n",
    "    print(f\"\\nSuccessfully processed {len(all_documents)} files into documents.\")\n",
    "    return all_documents'''\n",
    "\n",
    "def parse_markdown_for_metadata(directory: str, google_drive_path: str = None) -> List[Document]:\n",
    "    markdown_files = glob.glob(os.path.join(directory, '**/*.md'), recursive=True)\n",
    "    if not markdown_files:\n",
    "        return []\n",
    "\n",
    "    headers_to_split_on = [\n",
    "        (\"#\", \"Header 1\"),\n",
    "        (\"##\", \"Header 2\"),\n",
    "        (\"###\", \"Header 3\"),\n",
    "    ]\n",
    "\n",
    "    markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=300)\n",
    "    \n",
    "    all_documents = []\n",
    "\n",
    "    for filepath in tqdm(markdown_files, desc=\"Processing documents\"):\n",
    "        try:\n",
    "            with open(filepath, 'r', encoding='utf-8') as f:\n",
    "                markdown_text = f.read()\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            continue\n",
    "\n",
    "        filename_base = os.path.basename(filepath)\n",
    "        \n",
    "        # 1. Get filename without .md\n",
    "        name_no_ext = os.path.splitext(filename_base)[0]\n",
    "        \n",
    "        # 2. Clean name for metadata (remove underscores as requested)\n",
    "        # Result example: \"fhs.hr en course masthe b\"\n",
    "        doc_name_clean = name_no_ext.replace(\"_\", \" \") \n",
    "\n",
    "        md_header_splits = markdown_splitter.split_text(markdown_text)\n",
    "        final_splits = text_splitter.split_documents(md_header_splits)\n",
    "\n",
    "        for i, split in enumerate(final_splits):\n",
    "            doc = Document(page_content=split.page_content)\n",
    "            doc.metadata = split.metadata\n",
    "            doc.metadata[\"source\"] = filename_base # Keeps the original filename\n",
    "            doc.metadata[\"doc_name\"] = doc_name_clean # Searchable clean name\n",
    "            doc.metadata[\"source_path\"] = google_drive_path or filepath\n",
    "            doc.metadata[\"chunk_index\"] = i\n",
    "            \n",
    "            header_context = \" > \".join([v for k, v in split.metadata.items() if \"Header\" in k])\n",
    "            doc.metadata[\"header_path\"] = header_context\n",
    "            \n",
    "            all_documents.append(doc)\n",
    "\n",
    "    return all_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af21f7b0",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def create_collection(chroma_client, gemini_embedding_function, documents_list):\n",
    "    \"\"\"\n",
    "    Create or update ChromaDB collection with optimized batch processing.\n",
    "    \"\"\"\n",
    "    DB_NAME = \"hrstud-bot-en\"\n",
    "    embed_fn = gemini_embedding_function\n",
    "    embed_fn.document_mode = True\n",
    "\n",
    "    db = chroma_client.get_or_create_collection(\n",
    "        name=DB_NAME,\n",
    "        metadata={\"model\": \"models/text-embedding-004\", \"dimension\": 768},\n",
    "        embedding_function=embed_fn\n",
    "    )\n",
    "\n",
    "    documents = [doc.page_content for doc in documents_list]\n",
    "    metadatas = [doc.metadata for doc in documents_list]\n",
    "    ids = [f\"{DB_NAME}_doc_{i}\" for i in range(len(documents))]\n",
    "\n",
    "    if db.count() == 0:\n",
    "        print(f\"Adding {len(documents)} documents to ChromaDB collection: {DB_NAME}\")\n",
    "\n",
    "        # Optimized batch size for Gemini API\n",
    "        BATCH_SIZE = 100\n",
    "        \n",
    "        for i in tqdm(range(0, len(documents), BATCH_SIZE), desc=\"Adding documents\", unit=\"batch\"):\n",
    "            batch_end = min(i + BATCH_SIZE, len(documents))\n",
    "            db.add(\n",
    "                documents=documents[i:batch_end],\n",
    "                metadatas=metadatas[i:batch_end],\n",
    "                ids=ids[i:batch_end]\n",
    "            )\n",
    "            # Rate limiting for API stability\n",
    "            time.sleep(0.2)\n",
    "\n",
    "        print(f\"\\nCollection '{DB_NAME}' now contains {db.count()} documents.\")\n",
    "    else:\n",
    "        print(f\"Collection '{DB_NAME}' already has {db.count()} documents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891a657a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def persistent_client(embed_fn):\n",
    "    \"\"\"\n",
    "    Initialize persistent ChromaDB client.\n",
    "    \"\"\"\n",
    "    persist_dir = \"./output_en\"\n",
    "    chroma_client = chromadb.PersistentClient(path=persist_dir)\n",
    "\n",
    "    DB_NAME = \"hrstud-bot-en\"\n",
    "    collection = chroma_client.get_collection(DB_NAME, embedding_function=embed_fn)\n",
    "\n",
    "    print(f\"Connected to collection: {collection.name}\")\n",
    "    print(f\"Documents: {collection.count()}\")\n",
    "    print(f\"Metadata: {collection.metadata}\")\n",
    "    return embed_fn, collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05886a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_en(user_query, embed_fn, collection, client):\n",
    "    \n",
    "    embed_fn.document_mode = False\n",
    "    n_results_to_fetch = 7\n",
    "    result = collection.query(query_texts=[user_query], n_results=n_results_to_fetch)\n",
    "    \n",
    "    all_passages = result[\"documents\"][0]\n",
    "    all_metadatas = result[\"metadatas\"][0]\n",
    "\n",
    "    query_oneline = user_query.replace(\"\\n\", \" \")\n",
    "    print(query_oneline)\n",
    "    \n",
    "    # Extract the main source link (from the first/most relevant result)\n",
    "    main_source_link = all_metadatas[0].get(\"source_path\", \"Link not available\")\n",
    "    \n",
    "    # Construct context\n",
    "    context_list = []\n",
    "    source_links = []  # Collect all unique source links\n",
    "    \n",
    "    for i, (passage, metadata) in enumerate(zip(all_passages, all_metadatas)):\n",
    "        source_name = metadata.get(\"source\", \"Unknown source\")\n",
    "        source_path = metadata.get(\"source_path\", \"\")\n",
    "        \n",
    "        # Collect unique sources for the bottom reference\n",
    "        if source_path and source_path not in source_links:\n",
    "            source_links.append(source_path)\n",
    "        \n",
    "        context_list.append(f\"--- Source: {source_name} (Part {i+1} of {len(all_passages)}) ---\\n{passage.strip()}\")\n",
    "\n",
    "    context = \"\\n\\n\".join(context_list)\n",
    "    \n",
    "    # Format sources for the bottom of the answer\n",
    "    sources_text = \"\\n\".join([f\"- {link}\" for link in source_links])\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    You are a kind, precise, and informative chatbot of the **Faculty of Croatian Studies**. \n",
    "    Your main task is to answer questions from students, prospective students, and staff about the faculty, \n",
    "    including information about study programs, courses, departments, admissions, and general school information.\n",
    "\n",
    "    **CRITICAL RULES:**\n",
    "    1.  Use ONLY the information provided in the supplied documentation.\n",
    "    2.  Respond **in English**.\n",
    "    3.  Be concise but complete — **synthesize all relevant details from ALL context sources into ONE cohesive answer**.\n",
    "    4.  If the documentation does not contain the answer, clearly and politely state that you cannot find the answer \n",
    "        in the knowledge base and direct the user to contact the appropriate office.\n",
    "    5.  Note if some classes are not offered in English.\n",
    "    6.  **Do not use phrases like \"Of course, I can help you!\" or \"Here is some information about...\". \n",
    "        Start directly with the relevant answer.**\n",
    "    7.  **IMPORTANT: Provide ONE unified answer, not multiple separate responses for each source.**\n",
    "\n",
    "    **ANSWER FORMATTING:**\n",
    "    * If there is ONE main source, start with: **Source: [Main Source Link]** followed by a blank line.\n",
    "    * If there are MULTIPLE sources, provide the answer first, then at the end add a \"**Sources:**\" section listing all source links.\n",
    "    * Use bold text for key terms (e.g., **Admissions**, **Philosophy**, **Head of Department**).\n",
    "    * **When listing courses taught by a professor, organize them by level:**\n",
    "    - **Undergraduate Courses:**\n",
    "    - **Graduate Courses:**\n",
    "    - **Doctoral Courses:**\n",
    "    * For each course, include relevant details like ECTS credits, course hours, and language availability.\n",
    "    * Responses should be professional and formal, yet polite in tone.\n",
    "    * **Combine information from all sources into a single, coherent response.**\n",
    "    * **DO NOT repeat \"The source link is...\" multiple times. Use it ONCE at the top if there's one main source, OR list all sources at the bottom.**\n",
    "\n",
    "    **AVAILABLE DOCUMENTATION (Context):**\n",
    "    {context}\n",
    "\n",
    "    **USER QUESTION:** {query_oneline}\n",
    "\n",
    "    **INSTRUCTIONS FOR SOURCE CITATION:**\n",
    "    Main source link: {main_source_link}\n",
    "    All source links: {sources_text}\n",
    "\n",
    "    If the answer comes primarily from ONE source, start with \"**Source:** {main_source_link}\".\n",
    "    If the answer uses MULTIPLE sources, end your response with:\n",
    "\n",
    "    **Sources:**\n",
    "    {sources_text}\n",
    "\n",
    "    **ANSWER (provide ONE unified response with courses organized by academic level):**\n",
    "    \"\"\"\n",
    "    \n",
    "    answer = client.models.generate_content(\n",
    "        model=\"gemini-2.5-flash\",\n",
    "        contents=prompt,\n",
    "        config={\n",
    "            \"max_output_tokens\": 2048,\n",
    "            \"temperature\": 0.2,\n",
    "            \"top_p\": 0.9\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return answer.text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2725bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# USAGE EXAMPLE - Uncomment to run\n",
    "\n",
    "markdown_folder = \"./markdown_en\"\n",
    "# \n",
    "# # STEP 1: Parse and chunk documents (run once or when documents change)\n",
    "md_documents = parse_markdown_for_metadata(markdown_folder)\n",
    "# \n",
    "# # STEP 2: Create collection and add documents (run once)\n",
    "client = import_google_api()\n",
    "gemini_embedding_function = embedding_function(client)\n",
    "chroma_persistent_client = chromadb.PersistentClient(path=\"./output_en\")\n",
    "create_collection(chroma_persistent_client, gemini_embedding_function, md_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52583056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 3: Query the system (run for each query)\n",
    "# \n",
    "client = import_google_api()\n",
    "gemini_embedding_function = embedding_function(client)\n",
    "embed_fn, collection = persistent_client(gemini_embedding_function)\n",
    "# \n",
    "user_query = \"Who is Sandro Skansi?\"  # Example query\n",
    "response = get_article_en(\n",
    "    user_query=user_query,\n",
    "    embed_fn=embed_fn,\n",
    "    collection=collection,\n",
    "    client=client,\n",
    ")\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d872d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADVANCED: Test multiple queries\n",
    "# \n",
    "test_queries = [\n",
    "    \"What classes does Marko Jerković teach\",\n",
    "    \"What classes does Sandro Skansi teach?\",\n",
    "    \"Who teaches Medieval European History?\",\n",
    "    \"What History Undergraduate classes are offered in English?\",\n",
    "    \"What is the number of Snježana Konovski?\" # ili nema ili je premalo podataka na en stranici\n",
    "\n",
    "]\n",
    "# \n",
    "for query in test_queries:\n",
    "    print(f\"\\n{'#'*60}\")\n",
    "    print(f\"QUERY: {query}\")\n",
    "    print(f\"{'#'*60}\")\n",
    "    response = get_article_en(\n",
    "        user_query=query,\n",
    "        embed_fn=embed_fn,\n",
    "        collection=collection,\n",
    "        client=client,\n",
    "    )\n",
    "    display(Markdown(response))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6889c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADVANCED: Test multiple queries\n",
    "# \n",
    "test_queries = [\n",
    "    \"What History Undergraduate classes are offered in English?\",\n",
    "    \"What is the number of Snježana Konovski?\"\n",
    "\n",
    "]\n",
    "# \n",
    "for query in test_queries:\n",
    "    print(f\"\\n{'#'*60}\")\n",
    "    print(f\"QUERY: {query}\")\n",
    "    print(f\"{'#'*60}\")\n",
    "    response = get_article_en(\n",
    "        user_query=query,\n",
    "        embed_fn=embed_fn,\n",
    "        collection=collection,\n",
    "        client=client,\n",
    "    )\n",
    "    display(Markdown(response))\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
