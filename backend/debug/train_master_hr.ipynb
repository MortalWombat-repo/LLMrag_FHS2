{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf6f289b",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from IPython.display import Markdown\n",
    "from IPython.display import display\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import glob\n",
    "import chromadb\n",
    "from chromadb import Documents, EmbeddingFunction, Embeddings\n",
    "from google.api_core import retry\n",
    "from langcodes import Language\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from typing import List, Tuple\n",
    "import fasttext\n",
    "import re\n",
    "from typing import Any, Dict, List, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ad699fd",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def import_google_api():\n",
    "    load_dotenv()\n",
    "    GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "    client = genai.Client(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "    # Simple model check (keeping the original print logic)\n",
    "    for m in client.models.list():\n",
    "        if \"embedContent\" in m.supported_actions:\n",
    "            print(m.name)\n",
    "\n",
    "    return client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b4171d9",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def embedding_function(client):\n",
    "    class GeminiEmbeddingFunction(EmbeddingFunction):\n",
    "        document_mode = True\n",
    "\n",
    "        def __init__(self, client):\n",
    "            self.client = client\n",
    "            # Retry only on specific transient API errors\n",
    "            self._retry = retry.Retry(predicate=lambda e: isinstance(e, genai.errors.APIError) and e.code in {429, 503})\n",
    "\n",
    "        def __call__(self, input: Documents) -> Embeddings:\n",
    "            embedding_task = \"retrieval_document\" if self.document_mode else \"retrieval_query\"\n",
    "            response = self._retry(self.client.models.embed_content)(\n",
    "                model=\"models/text-embedding-004\",\n",
    "                contents=input,\n",
    "                config=types.EmbedContentConfig(task_type=embedding_task),\n",
    "            )\n",
    "            return [e.values for e in response.embeddings]\n",
    "\n",
    "    return GeminiEmbeddingFunction(client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34fb3f4f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Assuming 'Document' is a class/dataclass with 'page_content' and 'metadata' attributes\n",
    "# from a library like LangChain, LlamaIndex, etc.\n",
    "class Document:\n",
    "    def __init__(self, page_content: str, metadata: dict = None):\n",
    "        self.page_content = page_content\n",
    "        self.metadata = metadata if metadata is not None else {}\n",
    "\n",
    "# NOTE: The original doc_stats, filename_base, google_drive_path, and all_chunks \n",
    "# are not defined in the provided snippet. I'll define placeholders or \n",
    "# make assumptions for a runnable example.\n",
    "\n",
    "'''def parse_markdown_for_metadata(directory: str, google_drive_path: str = None) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Reads markdown files in a directory (and subdirectories) and creates a \n",
    "    single Document for each file, adding relevant metadata, but does not chunk.\n",
    "    \"\"\"\n",
    "    markdown_files = glob.glob(os.path.join(directory, '**/*.md'), recursive=True)\n",
    "    if not markdown_files:\n",
    "        print(\"No markdown files found\")\n",
    "        return []\n",
    "\n",
    "    print(f\"Processing {len(markdown_files)} markdown files...\")\n",
    "    \n",
    "    all_documents = []\n",
    "\n",
    "    for filepath in tqdm(markdown_files, desc=\"Processing documents\"):\n",
    "        try:\n",
    "            with open(filepath, 'r', encoding='utf-8') as f:\n",
    "                markdown_text = f.read()\n",
    "        except Exception as e:\n",
    "            print(f\"\\nWarning: Could not read file {filepath}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Extract basic file name for metadata\n",
    "        filename_base = os.path.basename(filepath)\n",
    "\n",
    "        # Create a single Document for the entire file content\n",
    "        doc = Document(page_content=markdown_text)\n",
    "\n",
    "        # Add metadata\n",
    "        doc.metadata[\"source\"] = filename_base\n",
    "        # Use google_drive_path if provided, otherwise use local path\n",
    "        doc.metadata[\"source_path\"] = google_drive_path or filepath \n",
    "        \n",
    "        # NOTE: Since we are not chunking by headers, we won't have a specific header.\n",
    "        # We set it to an empty string or a placeholder.\n",
    "        doc.metadata[\"header\"] = \"\" \n",
    "        \n",
    "        # Since the entire file is one document, these values reflect that.\n",
    "        doc.metadata[\"chunk_index\"] = 0\n",
    "        doc.metadata[\"total_chunks\"] = 1\n",
    "        doc.metadata[\"is_complete_doc\"] = True\n",
    "        \n",
    "        all_documents.append(doc)\n",
    "\n",
    "    print(f\"\\nSuccessfully processed {len(all_documents)} files into documents.\")\n",
    "    return all_documents'''\n",
    "'''def parse_markdown_for_metadata(directory: str, google_drive_path: str = None) -> List[Document]:\n",
    "    markdown_files = glob.glob(os.path.join(directory, '**/*.md'), recursive=True)\n",
    "    if not markdown_files:\n",
    "        return []\n",
    "\n",
    "    # 1. Definiraj zaglavlja koja želiš pratiti\n",
    "    headers_to_split_on = [\n",
    "        (\"#\", \"Header 1\"),\n",
    "        (\"##\", \"Header 2\"),\n",
    "        (\"###\", \"Header 3\"),\n",
    "    ]\n",
    "\n",
    "    # Inicijaliziraj splittere\n",
    "    markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "    \n",
    "    # Sekundarni splitter koji pazi na veličinu (da chunk ne bude prevelik za embedding)\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=800, \n",
    "        chunk_overlap=100\n",
    "    )\n",
    "    \n",
    "    all_documents = []\n",
    "\n",
    "    for filepath in tqdm(markdown_files, desc=\"Processing documents\"):\n",
    "        try:\n",
    "            with open(filepath, 'r', encoding='utf-8') as f:\n",
    "                markdown_text = f.read()\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            continue\n",
    "\n",
    "        # 2. Prvo splitaj po Markdown naslovima\n",
    "        md_header_splits = markdown_splitter.split_text(markdown_text)\n",
    "\n",
    "        # 3. Dodatno splitaj ako su sekcije preduge i dodaj metapodatke\n",
    "        final_splits = text_splitter.split_documents(md_header_splits)\n",
    "\n",
    "        for i, split in enumerate(final_splits):\n",
    "            filename_base = os.path.basename(filepath)\n",
    "            \n",
    "            # Kreiraj novi Document objekt (kompatibilan s tvojim ChromaDB setupom)\n",
    "            doc = Document(page_content=split.page_content)\n",
    "            \n",
    "            # Kopiraj metapodatke iz splittera (tu su sada Header 1, Header 2 itd.)\n",
    "            doc.metadata = split.metadata\n",
    "            doc.metadata[\"source\"] = filename_base\n",
    "            doc.metadata[\"source_path\"] = google_drive_path or filepath\n",
    "            doc.metadata[\"chunk_index\"] = i\n",
    "            \n",
    "            # Spoji naslove u jedan string radi lakše pretrage (opcionalno)\n",
    "            header_context = \" > \".join([v for k, v in split.metadata.items() if \"Header\" in k])\n",
    "            doc.metadata[\"header_path\"] = header_context\n",
    "            \n",
    "            all_documents.append(doc)\n",
    "\n",
    "    return all_documents'''\n",
    "'''def parse_markdown_for_metadata(directory: str, google_drive_path: str = None) -> List[Document]:\n",
    "    markdown_files = glob.glob(os.path.join(directory, '**/*.md'), recursive=True)\n",
    "    if not markdown_files:\n",
    "        return []\n",
    "\n",
    "    headers_to_split_on = [\n",
    "        (\"#\", \"Header 1\"),\n",
    "        (\"##\", \"Header 2\"),\n",
    "        (\"###\", \"Header 3\"),\n",
    "    ]\n",
    "\n",
    "    markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "    \n",
    "    # CHANGED: Increased chunk size and overlap\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=2000,    # Changed from 800\n",
    "        chunk_overlap=300   # Changed from 100\n",
    "    )\n",
    "    \n",
    "    all_documents = []\n",
    "\n",
    "    for filepath in tqdm(markdown_files, desc=\"Processing documents\"):\n",
    "        try:\n",
    "            with open(filepath, 'r', encoding='utf-8') as f:\n",
    "                markdown_text = f.read()\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            continue\n",
    "\n",
    "        md_header_splits = markdown_splitter.split_text(markdown_text)\n",
    "        final_splits = text_splitter.split_documents(md_header_splits)\n",
    "\n",
    "        for i, split in enumerate(final_splits):\n",
    "            filename_base = os.path.basename(filepath)\n",
    "            \n",
    "            doc = Document(page_content=split.page_content)\n",
    "            \n",
    "            doc.metadata = split.metadata\n",
    "            doc.metadata[\"source\"] = filename_base\n",
    "            doc.metadata[\"source_path\"] = google_drive_path or filepath\n",
    "            doc.metadata[\"chunk_index\"] = i\n",
    "            \n",
    "            header_context = \" > \".join([v for k, v in split.metadata.items() if \"Header\" in k])\n",
    "            doc.metadata[\"header_path\"] = header_context\n",
    "            \n",
    "            all_documents.append(doc)\n",
    "\n",
    "    return all_documents'''\n",
    "def parse_markdown_for_metadata(directory: str, google_drive_path: str = None) -> List[Document]:\n",
    "    markdown_files = glob.glob(os.path.join(directory, '**/*.md'), recursive=True)\n",
    "    if not markdown_files:\n",
    "        return []\n",
    "\n",
    "    headers_to_split_on = [(\"#\", \"Header 1\"), (\"##\", \"Header 2\"), (\"###\", \"Header 3\")]\n",
    "    markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=300)\n",
    "    \n",
    "    all_documents = []\n",
    "\n",
    "    for filepath in tqdm(markdown_files, desc=\"Processing documents\"):\n",
    "        try:\n",
    "            with open(filepath, 'r', encoding='utf-8') as f:\n",
    "                markdown_text = f.read()\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            continue\n",
    "\n",
    "        filename_base = os.path.basename(filepath)\n",
    "        name_no_ext = os.path.splitext(filename_base)[0]\n",
    "        \n",
    "        # Strip domain and underscores for cleaner metadata\n",
    "        clean_id = name_no_ext.replace(\"fhs.hr_\", \"\") \n",
    "        doc_name_clean = clean_id.replace(\"_\", \" \") \n",
    "\n",
    "        md_header_splits = markdown_splitter.split_text(markdown_text)\n",
    "        final_splits = text_splitter.split_documents(md_header_splits)\n",
    "\n",
    "        for i, split in enumerate(final_splits):\n",
    "            # 1. Generate the header context first so we can use it in searchable_content\n",
    "            header_context = \" > \".join([v for k, v in split.metadata.items() if \"Header\" in k])\n",
    "            \n",
    "            # 2. THE ACCURACY BOOST: Prepend metadata to the content\n",
    "            # This ensures names and sections are indexed by the vector search\n",
    "            searchable_content = f\"Dokument: {doc_name_clean}\\nSekcija: {header_context if header_context else 'Opće'}\\n\\n{split.page_content}\"\n",
    "            \n",
    "            # 3. Create the Document with the enriched content\n",
    "            doc = Document(page_content=searchable_content)\n",
    "            \n",
    "            # 4. Standard Metadata Assignments\n",
    "            doc.metadata = split.metadata\n",
    "            doc.metadata[\"source\"] = filename_base \n",
    "            doc.metadata[\"article_link\"] = clean_id \n",
    "            doc.metadata[\"doc_name\"] = doc_name_clean \n",
    "            doc.metadata[\"source_path\"] = google_drive_path or filepath\n",
    "            doc.metadata[\"chunk_index\"] = i\n",
    "            doc.metadata[\"header_path\"] = header_context\n",
    "            \n",
    "            all_documents.append(doc)\n",
    "\n",
    "    return all_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af21f7b0",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def create_collection(chroma_client, gemini_embedding_function, documents_list):\n",
    "    \"\"\"\n",
    "    Create or update ChromaDB collection with optimized batch processing.\n",
    "    \"\"\"\n",
    "    DB_NAME = \"hrstud-bot-hr\"\n",
    "    embed_fn = gemini_embedding_function\n",
    "    embed_fn.document_mode = True\n",
    "\n",
    "    db = chroma_client.get_or_create_collection(\n",
    "        name=DB_NAME,\n",
    "        metadata={\"model\": \"models/text-embedding-004\", \"dimension\": 768},\n",
    "        embedding_function=embed_fn\n",
    "    )\n",
    "\n",
    "    documents = [doc.page_content for doc in documents_list]\n",
    "    metadatas = [doc.metadata for doc in documents_list]\n",
    "    ids = [f\"{DB_NAME}_doc_{i}\" for i in range(len(documents))]\n",
    "\n",
    "    if db.count() == 0:\n",
    "        print(f\"Adding {len(documents)} documents to ChromaDB collection: {DB_NAME}\")\n",
    "\n",
    "        # Optimized batch size for Gemini API\n",
    "        BATCH_SIZE = 100\n",
    "        \n",
    "        for i in tqdm(range(0, len(documents), BATCH_SIZE), desc=\"Adding documents\", unit=\"batch\"):\n",
    "            batch_end = min(i + BATCH_SIZE, len(documents))\n",
    "            db.add(\n",
    "                documents=documents[i:batch_end],\n",
    "                metadatas=metadatas[i:batch_end],\n",
    "                ids=ids[i:batch_end]\n",
    "            )\n",
    "            # Rate limiting for API stability\n",
    "            time.sleep(0.2)\n",
    "\n",
    "        print(f\"\\nCollection '{DB_NAME}' now contains {db.count()} documents.\")\n",
    "    else:\n",
    "        print(f\"Collection '{DB_NAME}' already has {db.count()} documents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "891a657a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def persistent_client(embed_fn):\n",
    "    \"\"\"\n",
    "    Initialize persistent ChromaDB client.\n",
    "    \"\"\"\n",
    "    persist_dir = \"./output_hr\"\n",
    "    chroma_client = chromadb.PersistentClient(path=persist_dir)\n",
    "\n",
    "    DB_NAME = \"hrstud-bot-hr\"\n",
    "    collection = chroma_client.get_collection(DB_NAME, embedding_function=embed_fn)\n",
    "\n",
    "    print(f\"Connected to collection: {collection.name}\")\n",
    "    print(f\"Documents: {collection.count()}\")\n",
    "    print(f\"Metadata: {collection.metadata}\")\n",
    "    return embed_fn, collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "05886a06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "# NOTE: Placeholder for helper function (used in the original snippet)\n",
    "def _no_answer_response():\n",
    "    \"\"\"Standard no-answer response.\"\"\"\n",
    "    return (\"Ispričavamo se, ali ne mogu pronaći relevantan odgovor u bazi znanja. \"\n",
    "            \"Molimo kontaktirajte odgovarajuću službu za dodatne informacije.\")\n",
    "\n",
    "# FastText model loading assumed to be successful from the previous block\n",
    "LID_MODEL = fasttext.load_model('./fasttext/lid.176.ftz') \n",
    "\n",
    "'''def get_article_hr(user_query, embed_fn, collection, client):\n",
    "    \n",
    "    # Switch to query mode when generating embeddings\n",
    "    embed_fn.document_mode = False\n",
    "\n",
    "    # Retrieve top 1 document (based on your n_results=1 in the original code)\n",
    "    # The result structure is a dict: {'ids': [[]], 'distances': [[]], 'documents': [[]], 'metadatas': [[]], ...}\n",
    "    n_results_to_fetch = 30 # Fetch more results for a richer context\n",
    "    result = collection.query(query_texts=[user_query], n_results=n_results_to_fetch)\n",
    "    \n",
    "    # Extract documents (list of passages) and metadatas (list of dicts)\n",
    "    all_passages = result[\"documents\"][0]\n",
    "    all_metadatas = result[\"metadatas\"][0]\n",
    "\n",
    "    query_oneline = user_query.replace(\"\\n\", \" \")\n",
    "    print(query_oneline)\n",
    "    \n",
    "    # 1. CONSTRUCT THE CONTEXT\n",
    "    context_list = []\n",
    "    # Use the metadata from the top result to define the main source link\n",
    "    # Assuming 'source_path' contains the URL or relevant file path\n",
    "    #document_link = all_metadatas[0].get(\"source_path\", \"Link nije dostupan\")\n",
    "    \n",
    "    for i, (passage, metadata) in enumerate(zip(all_passages, all_metadatas)):\n",
    "        # Format the context for the model\n",
    "        source_name = metadata.get(\"source\", \"Nepoznat izvor\")\n",
    "        # I removed the redundant \"PASSAGE: \" wrapper that was causing issues\n",
    "        context_list.append(f\"--- Izvor: {source_name} (Dio {i+1} od {len(all_passages)}) ---\\n{passage.strip()}\")\n",
    "\n",
    "    # Join all context chunks into a single string\n",
    "    context = \"\\n\\n\".join(context_list)\n",
    "    \n",
    "    # 2. CONSTRUCT THE PROMPT\n",
    "    # The document_link is now a defined variable\n",
    "    prompt = f\"\"\"\n",
    "    Ti si ljubazan, precizan i informativan chatbot **Fakulteta Hrvatskih studija**. Tvoja je glavna zadaća odgovarati na pitanja studenata, potencijalnih studenata i osoblja o fakultetu, uključujući informacije o studijima, nastavi, smjerovima, prijavama, i općenitim informacijama o školi.\n",
    "\n",
    "    **KRITIČNA PRAVILA:**\n",
    "    1.  Koristi ISKLJUČIVO informacije iz dostavljene dokumentacije.\n",
    "    2.  Odgovaraj na **Hrvatskom jeziku**.\n",
    "    3.  Budi koncizan ali potpun — navedi sve relevantne detalje iz konteksta.\n",
    "    4.  Ako dokumentacija ne sadrži odgovor, jasno i ljubazno reci da ne možeš pronaći odgovor u bazi znanja i uputi na kontaktiranje odgovarajuće službe.\n",
    "    5.  **Ne smiješ koristiti fraze poput \"Naravno, mogu vam pomoći!\" ili \"Evo nekoliko informacija o...\". Odmah započni s relevantnim odgovorom.**\n",
    "\n",
    "    **FORMATIRANJE ODGOVORA:**\n",
    "    * Sve odgovore započni s **Izvorni link je [LINK](url)**, nakon čega slijedi prazan red.\n",
    "    * Nemoj navoditi izvorni link dokumenta samo URL. npr nemoj navoditi: Izvorni link: ./markdown/fhs.hr_predmet_opsv.md\n",
    "    * Koristi podebljani tekst za ključne pojmove (npr. **Upisi**, **Filozofija**, **Pročelnik**).\n",
    "    * Koristi popise (liste) za nabrajanje informacija (studiji, uvjeti, rokovi).\n",
    "    * Odgovori trebaju biti profesionalni i službeni, ali s ljubaznim tonom.\n",
    "\n",
    "    **DOSTUPNA DOKUMENTACIJA (Kontekst):**\n",
    "    {context}\n",
    "\n",
    "    **KORISNIČKO PITANJE:** {query_oneline}\n",
    "\n",
    "    **ODGOVOR:**\n",
    "    \"\"\"\n",
    "    \n",
    "    # 3. Call the model\n",
    "    answer = client.models.generate_content(\n",
    "        model=\"gemini-2.5-flash\",\n",
    "        contents=prompt, # Use the full prompt\n",
    "        config={\n",
    "            \"max_output_tokens\": 2048,\n",
    "            \"temperature\": 0.2,\n",
    "            \"top_p\": 0.9\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Prepend the link as per your strict instruction, since Gemini might not format the first line perfectly\n",
    "    #final_response = f\"Izvorni link: {document_link}\\n\\n{answer.text.strip()}\"\n",
    "    \n",
    "    #return final_response\n",
    "\n",
    "    return answer.text.strip()'''\n",
    "\n",
    "'''def get_article_hr(user_query, embed_fn, collection, client):\n",
    "    \n",
    "    # Switch to query mode when generating embeddings\n",
    "    embed_fn.document_mode = False\n",
    "\n",
    "    # Retrieve top N documents for richer context\n",
    "    n_results_to_fetch = 10\n",
    "    result = collection.query(query_texts=[user_query], n_results=n_results_to_fetch)\n",
    "    \n",
    "    # Extract documents (list of passages) and metadatas (list of dicts)\n",
    "    all_passages = result[\"documents\"][0]\n",
    "    all_metadatas = result[\"metadatas\"][0]\n",
    "    all_distances = result[\"distances\"][0]\n",
    "\n",
    "    query_oneline = user_query.replace(\"\\n\", \" \")\n",
    "    print(f\"Query: {query_oneline}\")\n",
    "    print(f\"Top 5 results (distances): {all_distances[:5]}\")\n",
    "    \n",
    "    # 1. CONSTRUCT THE CONTEXT with source tracking\n",
    "    context_list = []\n",
    "    sources_used = set()  # Track unique sources\n",
    "    \n",
    "    for i, (passage, metadata, distance) in enumerate(zip(all_passages, all_metadatas, all_distances)):\n",
    "        source_name = metadata.get(\"source\", \"Nepoznat izvor\")\n",
    "        source_path = metadata.get(\"source_path\", \"\")\n",
    "        header_path = metadata.get(\"header_path\", \"\")\n",
    "        \n",
    "        # Track sources for final reference\n",
    "        sources_used.add((source_name, source_path))\n",
    "        \n",
    "        # Format context with all relevant metadata\n",
    "        context_entry = f\"\"\"--- Izvor {i+1}: {source_name} ---\n",
    "Sekcija: {header_path if header_path else \"Opće informacije\"}\n",
    "Relevantnost: {distance:.3f}\n",
    "\n",
    "{passage.strip()}\"\"\"\n",
    "        \n",
    "        context_list.append(context_entry)\n",
    "\n",
    "    # Join all context chunks\n",
    "    context = \"\\n\\n\".join(context_list)\n",
    "    \n",
    "    # Format sources for the model to reference\n",
    "    sources_formatted = \"\\n\".join([f\"- {name}: {path}\" for name, path in sorted(sources_used)])\n",
    "    \n",
    "    # 2. CONSTRUCT THE PROMPT\n",
    "    prompt = f\"\"\"Ti si ljubazan, precizan i informativan chatbot **Fakulteta Hrvatskih studija**. Tvoja je glavna zadaća odgovarati na pitanja studenata, potencijalnih studenata i osoblja o fakultetu.\n",
    "\n",
    "**KRITIČNA PRAVILA:**\n",
    "1. Koristi ISKLJUČIVO informacije iz dostavljene dokumentacije.\n",
    "2. Odgovaraj na **Hrvatskom jeziku**.\n",
    "3. Budi koncizan ali potpun — navedi sve relevantne detalje iz konteksta.\n",
    "4. Ako dokumentacija ne sadrži odgovor, jasno reci da ne možeš pronaći odgovor u bazi znanja.\n",
    "5. **Ne koristi fraze poput \"Naravno, mogu vam pomoći!\" - odmah započni s relevantnim odgovorom.**\n",
    "\n",
    "**FORMATIRANJE ODGOVORA:**\n",
    "* Na POČETKU odgovora, u prvoj liniji, navedi izvor(e) u formatu: **Izvor:** [naziv dokumenta]\n",
    "* Nakon izvora ostavi prazan red, pa nastavi s odgovorom.\n",
    "* Koristi podebljani tekst za ključne pojmove (npr. **ZET linija 215**, **Kampus Borongaj**).\n",
    "* Koristi popise za nabrajanje informacija.\n",
    "* Ton: profesionalan i ljubazan.\n",
    "\n",
    "**DOSTUPNI IZVORI:**\n",
    "{sources_formatted}\n",
    "\n",
    "**KONTEKST (30 najrelevantnijih dijelova dokumentacije):**\n",
    "{context}\n",
    "\n",
    "**KORISNIČKO PITANJE:** {query_oneline}\n",
    "\n",
    "**TVOJ ODGOVOR (započni s **Izvor:** na prvoj liniji):**\"\"\"\n",
    "    \n",
    "    # 3. Call the model\n",
    "    answer = client.models.generate_content(\n",
    "        model=\"gemini-2.5-flash\",\n",
    "        contents=prompt,\n",
    "        config={\n",
    "            \"max_output_tokens\": 2048,\n",
    "            \"temperature\": 0.2,\n",
    "            \"top_p\": 0.9\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    return answer.text.strip()'''\n",
    "import re\n",
    "from typing import List\n",
    "\n",
    "def get_article_hr(user_query, embed_fn, collection, client):\n",
    "    # 1. PRIPREMA I QUERY EXPANSION\n",
    "    embed_fn.document_mode = False\n",
    "    query_lower = user_query.lower()\n",
    "    \n",
    "    # Inicijalne vrijednosti\n",
    "    expanded_query = user_query\n",
    "    n_results_to_fetch = 12\n",
    "\n",
    "    # Logika za profesore i kolegije\n",
    "    if any(word in query_lower for word in [\"predaje\", \"tko\", \"nastavnik\", \"profesor\", \"kolegij\"]):\n",
    "        expanded_query = f\"{user_query} profesor nositelj zvanje nastava kolegij studij\"\n",
    "    \n",
    "    # Logika za lokaciju\n",
    "    elif any(word in query_lower for word in [\"doći\", \"lokacija\", \"gdje\", \"kampus\", \"borongaj\", \"autobus\", \"vlak\"]):\n",
    "        expanded_query = f\"{user_query} lokacija adresa kampus borongaj autobus 215 236 vlak stanica Trnava\"\n",
    "\n",
    "    # Logika za studije (Povećavamo broj rezultata na 25 jer su studiji raspršeni po datotekama)\n",
    "    if any(word in query_lower for word in [\"studij\", \"nudite\", \"program\", \"upisi\", \"smjer\"]):\n",
    "        expanded_query = f\"{user_query} popis svih studija prijediplomski diplomski doktorski studij kroatologija povijest sociologija psihologija komunikologija filozofija\"\n",
    "        n_results_to_fetch = 20\n",
    "    \n",
    "    # 2. VECTOR SEARCH (Sada koristimo ispravan expanded_query i n_results)\n",
    "    result = collection.query(\n",
    "        query_texts=[expanded_query], \n",
    "        n_results=n_results_to_fetch\n",
    "    )\n",
    "    \n",
    "    all_passages = result[\"documents\"][0]\n",
    "    all_metadatas = result[\"metadatas\"][0]\n",
    "    all_distances = result[\"distances\"][0]\n",
    "\n",
    "    # 3. DEDUPLIKACIJA I PRIPREMA KONTEKSTA\n",
    "    context_list = []\n",
    "    seen_passages = set()\n",
    "    \n",
    "    for p, m, d in zip(all_passages, all_metadatas, all_distances):\n",
    "        # Nešto labaviji prag (0.90) za studije kako ne bismo propustili neki odsjek\n",
    "        if d < 0.90: \n",
    "            fingerprint = p.strip()[:150] \n",
    "            if fingerprint in seen_passages:\n",
    "                continue\n",
    "            seen_passages.add(fingerprint)\n",
    "            \n",
    "            chunk_url = m.get(\"article_link\", \"\")\n",
    "            context_list.append(f\"Izvor URL: {chunk_url}\\n{p.strip()}\")\n",
    "\n",
    "    if not context_list:\n",
    "        return \"Nažalost, ne mogu pronaći informacije o vašem upitu u bazi znanja.\"\n",
    "\n",
    "    context = \"\\n\\n\".join(context_list)\n",
    "    query_oneline = user_query.replace(\"\\n\", \" \")\n",
    "\n",
    "    # 4. IZVLAČENJE GLAVNOG LINKA\n",
    "    main_url = all_metadatas[0].get(\"article_link\", \"https://www.fhs.hr\")\n",
    "    article_link_markdown = f\"[Article Link]({main_url})\"\n",
    "\n",
    "    # 5. PROMPT ZA GEMINI (Postavljen na 2.0 Flash ili 2.0 Flash-Lite)\n",
    "    prompt = f\"\"\"\n",
    "    Ti si ljubazan, precizan i informativan chatbot **Fakulteta Hrvatskih studija**. Tvoja je zadaća odgovarati na pitanja o fakultetu.\n",
    "\n",
    "    **KRITIČNA PRAVILA:**\n",
    "    1. Koristi ISKLJUČIVO dostavljenu dokumentaciju (KONTEKST). Ako se informacija o studiju nalazi u kontekstu, navedi je.\n",
    "    2. Odgovaraj na **Hrvatskom jeziku**.\n",
    "    3. **GRUPIRANJE:** Ako ista osoba predaje više kolegija, navedi ime osobe SAMO JEDNOM. Grupiraj studije po razinama (Prijediplomski, Diplomski, Doktorski).\n",
    "    4. **POVEZNICE:** Kolegije, emailove i studije prikaži kao Markdown poveznice (npr. [Naziv](URL)) koristeći URL-ove iz konteksta.\n",
    "    5. **BEZ UVODA:** Odmah započni s relevantnim odgovorom.\n",
    "\n",
    "    **FORMATIRANJE ODGOVORA:**\n",
    "    * Prva linija odgovora: **Izvor:** {article_link_markdown}\n",
    "    * Nakon toga prazan red.\n",
    "    * Koristi **podebljani tekst** za ključne pojmove.\n",
    "    * Koristi liste (bullet points).\n",
    "\n",
    "    **DOSTUPNA DOKUMENTACIJA (Kontekst):**\n",
    "    {context}\n",
    "\n",
    "    **KORISNIČKO PITANJE:** {query_oneline}\n",
    "\n",
    "    **ODGOVOR:**\n",
    "    \"\"\"\n",
    "    \n",
    "    # 6. GENERIRANJE ODGOVORA\n",
    "    answer = client.models.generate_content(\n",
    "        model=\"gemini-2.5-flash-lite\", # Ili \"gemini-2.0-flash-lite\"\n",
    "        contents=prompt,\n",
    "        config={\n",
    "            \"max_output_tokens\": 2048,\n",
    "            \"temperature\": 0.1,\n",
    "            \"top_p\": 0.9\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    return answer.text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7e2725bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing documents: 100%|██████████| 1095/1095 [00:00<00:00, 1216.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/embedding-001\n",
      "models/text-embedding-004\n",
      "models/gemini-embedding-exp-03-07\n",
      "models/gemini-embedding-exp\n",
      "models/gemini-embedding-001\n",
      "Collection 'hrstud-bot-hr' already has 6776 documents.\n"
     ]
    }
   ],
   "source": [
    "# USAGE EXAMPLE - Uncomment to run\n",
    "\n",
    "markdown_folder = \"./markdown_hr\"\n",
    "# \n",
    "# # STEP 1: Parse and chunk documents (run once or when documents change)\n",
    "md_documents = parse_markdown_for_metadata(markdown_folder)\n",
    "# \n",
    "# # STEP 2: Create collection and add documents (run once)\n",
    "client = import_google_api()\n",
    "gemini_embedding_function = embedding_function(client)\n",
    "chroma_persistent_client = chromadb.PersistentClient(path=\"./output_hr\")\n",
    "create_collection(chroma_persistent_client, gemini_embedding_function, md_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "52583056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/embedding-001\n",
      "models/text-embedding-004\n",
      "models/gemini-embedding-exp-03-07\n",
      "models/gemini-embedding-exp\n",
      "models/gemini-embedding-001\n",
      "Connected to collection: hrstud-bot-hr\n",
      "Documents: 6776\n",
      "Metadata: {'dimension': 768, 'model': 'models/text-embedding-004'}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Izvor: [Article Link](studiji_upis_na_fhs_razredbeni_postupak)\n",
       "\n",
       "*   **Razredbeni postupak** za upis na studije Fakulteta hrvatskih studija opisan je na poveznici: [https://www.fhs.hr/studiji/upis_na_fhs/razredbeni_postupak](https://www.fhs.hr/studiji/upis_na_fhs/razredbeni_postupak)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# STEP 3: Query the system (run for each query)\n",
    "# \n",
    "client = import_google_api()\n",
    "gemini_embedding_function = embedding_function(client)\n",
    "embed_fn, collection = persistent_client(gemini_embedding_function)\n",
    "# \n",
    "#user_query = \"Tko je Sandro Skansi?\"  # Example query\n",
    "user_query = \"razredbeni postupak?\"  # Example query\n",
    "response = get_article_hr(\n",
    "    user_query=user_query,\n",
    "    embed_fn=embed_fn,\n",
    "    collection=collection,\n",
    "    client=client\n",
    ")\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d872d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# ADVANCED: Test multiple queries\n",
    "# \n",
    "test_queries = [\n",
    "    \"Koje predmete predaje Mato Škerbić?\",\n",
    "    \"Koje predmete predaje Sandro Skansi?\",\n",
    "    \"Tko predaje Opća povijest srednjeg vijeka?\"\n",
    "]\n",
    "# \n",
    "for query in test_queries:\n",
    "    print(f\"\\n{'#'*60}\")\n",
    "    print(f\"QUERY: {query}\")\n",
    "    print(f\"{'#'*60}\")\n",
    "    response = get_article_hr(\n",
    "        user_query=query,\n",
    "        embed_fn=embed_fn,\n",
    "        collection=collection,\n",
    "        client=client\n",
    "    )\n",
    "    display(Markdown(response))\n",
    "    print(\"\\n\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87afff21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADVANCED: Test multiple queries\n",
    "# \n",
    "test_queries = [\n",
    "    #\"Tko predaje Opća povijest srednjeg vijeka?\"\n",
    "    \"Koje predmete predaje Mato Škerbić?\"\n",
    "\n",
    "]\n",
    "# \n",
    "for query in test_queries:\n",
    "    print(f\"\\n{'#'*60}\")\n",
    "    print(f\"QUERY: {query}\")\n",
    "    print(f\"{'#'*60}\")\n",
    "    response = get_article_hr(\n",
    "        user_query=query,\n",
    "        embed_fn=embed_fn,\n",
    "        collection=collection,\n",
    "        client=client,\n",
    "    )\n",
    "    display(Markdown(response))\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
